## pick a specific version, to ensure predictability
FROM jupyter/pyspark-notebook@sha256:64420e4c348ab48fb806f42332109cbc205ae74cda67240c3e0974c5f7e6e969
## or latest
#FROM jupyter/pyspark-notebook@latest

MAINTAINER Intel BigDL Project <bigdl@intel.com>

## --- CONFIG
ARG MAVEN_VERSION=3.5.0
ARG SCALA_VERSION=2.11.8
ARG SPARK_VERSION=2.1.1
ARG SBT_VERSION=1.0.2
ARG INSTALL_DIR=/usr/local
## --- end CONFIG

USER root

## apt update
RUN apt-get update -yq && \
    apt-get -yq dist-upgrade

## basic utils + jdk
RUN apt-get install -yq  --no-install-recommends \
    atop \
    curl \
    less \
    openjdk-8-jdk-headless \
    rsync \
    unzip \
    wget \
    zip

## install maven
RUN curl -fsL  http://apache.cs.utah.edu/maven/maven-3/${MAVEN_VERSION}/binaries/apache-maven-${MAVEN_VERSION}-bin.tar.gz | tar xfz - -C ${INSTALL_DIR}

RUN cd ${INSTALL_DIR} && rm -f maven && ln -s apache-maven-${MAVEN_VERSION}  maven

ENV MAVEN_HOME ${INSTALL_DIR}/maven
ENV PATH=$MAVEN_HOME/bin:$PATH


## Scala expects this file
RUN touch /usr/lib/jvm/java-8-openjdk-amd64/release

## Install Scala
RUN \
  curl -fsL https://downloads.typesafe.com/scala/${SCALA_VERSION}/scala-${SCALA_VERSION}.tgz | tar xfz - -C ${INSTALL_DIR}

RUN cd ${INSTALL_DIR} && rm -f scala && ln -s scala-${SCALA_VERSION}  scala

ENV PATH=${INSTALL_DIR}/scala/bin:$PATH

## Install sbt
RUN \
  curl -L -o sbt-${SBT_VERSION}.deb "https://dl.bintray.com/sbt/debian/sbt-${SBT_VERSION}.deb" && \
  dpkg -i sbt-${SBT_VERSION}.deb && \
  rm sbt-${SBT_VERSION}.deb && \
  apt-get update && \
  apt-get install sbt && \
  sbt sbtVersion

## install spark
RUN \
  curl -fsL "https://d3kbcqa49mib13.cloudfront.net/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz" | tar xfz - -C ${INSTALL_DIR} && \
  cd ${INSTALL_DIR} &&  rm -f spark && ln -s spark-${SPARK_VERSION}-bin-hadoop2.7  spark

ENV APACHE_SPARK_VERSION=${SPARK_VERSION}

## cleanup apt
RUN apt-get clean && \
    rm -rf /var/lib/apt/lists/*

## ----- install BigDL
## This is a pre-built BigDL binary
RUN \
    cd ${INSTALL_DIR} && \
    wget --quiet "https://s3.amazonaws.com/elephantscale-public/BigDL/BigDL.zip"  && \
    unzip BigDL.zip && \
    rm -f BigDL.zip

ENV BIGDL_HOME  ${INSTALL_DIR}/BigDL

## ---- done with installs ----

## disable sudo password
RUN echo "${NB_USER} ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers

ENV PATH=${INSTALL_DIR}/spark/bin:$PATH

RUN mv /usr/local/bin/start-notebook.sh   /usr/local/bin/start-notebook-old.sh
COPY  start-notebook.sh  /usr/local/bin/
RUN chmod +x /usr/local/bin/start-notebook.sh

RUN mkdir /work

## ---- done with setup as root ----

### now as a regular user
USER $NB_USER

## update conda
# RUN conda update --all

## install NLTK
RUN  conda  install -y  nltk

# working directory
ENV WORKING_DIR ${HOME}/work
RUN rm -rf ${WORKING_DIR} && ln -s /work  ${WORKING_DIR}

## The following is to test builds
## commented out
#RUN mkdir -p ${WORKING_DIR}/src
## Clone bigdl-labs
#RUN cd ${WORKING_DIR} && git clone https://github.com/elephantscale/bigdl-tutorials
## clone BigDL repo, in case we want to build it
#RUN cd ${WORKING_DIR}/src && git clone https://github.com/intel-analytics/BigDL

COPY run_jupyter.sh  $HOME/

## finally switch back to jovyan to avoid accidental container runs as root
USER $NB_USER
