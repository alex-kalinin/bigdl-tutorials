{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.dataset.base import *\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from utils import *\n",
    "from bigdl.models.ml_pipeline.dl_classifier import *\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import  Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MultiClassClassificationEvaluator\n",
    "\n",
    "\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "training_epochs = 25\n",
    "batch_size = 16\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 4\n",
    "n_classes = 3\n",
    "n_hidden_1 = 10 # 1st layer number of features\n",
    "n_hidden_2 = 20 # 2nd layer number of features\n",
    "n_hidden_3 = 30 # 3rd layer number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_training = spark.read.csv(\"../data/iris/iris_training.csv\", header=True, inferSchema=\"true\", mode=\"DROPMALFORMED\")\n",
    "iris_test = spark.read.csv(\"../data/iris/iris_test.csv\", header=True, inferSchema=\"true\", mode=\"DROPMALFORMED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_training = iris_training.select([col(c).cast(\"double\") for c in iris_training.columns])\n",
    "iris_test = iris_test.select([col(c).cast(\"double\") for c in iris_test.columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler =  VectorAssembler(inputCols=['c1','c2','c3','c4'], outputCol=\"assembled\")\n",
    "scaler = StandardScaler(inputCol=\"assembled\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages = [assembler, scaler])\n",
    "pipelineTraining = pipeline.fit(iris_training)\n",
    "iris_data_training = pipelineTraining.transform(iris_training)\n",
    "pipelineTest = pipeline.fit(iris_test)\n",
    "iris_data_test = pipelineTraining.transform(iris_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+-----+\n",
      "|features                                                                    |label|\n",
      "+----------------------------------------------------------------------------+-----+\n",
      "|[7.3683612551017434,6.554983394668502,3.07337626655598,2.813157930275381]   |3.0  |\n",
      "|[5.7565322305482365,5.384450645620555,1.8110967285062027,1.2787081501251731]|2.0  |\n",
      "|[5.641401585937273,5.8526637452397345,2.469677357053913,2.1738038552127943] |3.0  |\n",
      "|[5.641401585937273,7.257303044097271,0.8232257856846377,0.12787081501251732]|1.0  |\n",
      "|[6.56244674282499,8.896048892764396,0.9329892237759226,0.38361244503755193] |1.0  |\n",
      "|[5.065748362882449,7.491409593906861,0.7134623475933526,0.25574163002503464]|1.0  |\n",
      "|[6.2170548089920965,7.959622693526039,0.8232257856846377,0.5114832600500693]|1.0  |\n",
      "|[7.944014478156568,7.257303044097271,2.798967671327768,2.941028745287898]   |3.0  |\n",
      "|[7.713753188934637,7.257303044097271,2.4147956380082705,1.7901914101752423] |2.0  |\n",
      "|[5.871662875159201,8.661942342954807,0.8232257856846377,0.5114832600500693] |1.0  |\n",
      "+----------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_data_training.select('features', 'label').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createLinear\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n",
      "creating: createClassNLLCriterion\n",
      "creating: createDLClassifier\n",
      "\n",
      "initial model training finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bigDLModel = Sequential().add(Linear(n_input, 10)).add(Linear(10, n_classes)).add(LogSoftMax())\n",
    "classnll_criterion = ClassNLLCriterion()\n",
    "dlClassifier = DLClassifier(model=bigDLModel, criterion=classnll_criterion, feature_size=[n_input])\n",
    "dlClassifier.setLabelCol(\"label\").setMaxEpoch(100).setBatchSize(8)\n",
    "model = dlClassifier.fit(iris_data_training)\n",
    "print(\"\\ninitial model training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictionDF = model.transform(iris_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the prediction and label column back to {0, 1}\n",
    "evaluateModel(predictionDF: DataFrame): Unit = {\n",
    "  predictionDF.cache()\n",
    "     \n",
    "  val metrics = new BinaryClassificationEvaluator().setRawPredictionCol(\"prediction\").setLabelCol(\"Class\")\n",
    "  val auPRC = metrics.evaluate(finalData)\n",
    "  println(\"\\nArea under precision-recall curve: = \" + auPRC)\n",
    "    \n",
    "  val recall = new MulticlassClassificationEvaluator().setLabelCol(\"Class\").setMetricName(\"weightedRecall\").evaluate(finalData)\n",
    "  println(\"\\nrecall = \" + recall)\n",
    "\n",
    "  val precisoin = new MulticlassClassificationEvaluator().setLabelCol(\"Class\").setMetricName(\"weightedPrecision\").evaluate(finalData)\n",
    "  println(\"\\nPrecision = \" + precisoin)  \n",
    "  predictionDF.unpersist()\n",
    "}\n",
    "\n",
    "evaluateModel(predictionDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.array(train_summary.read_scalar(\"Loss\"))\n",
    "top1 = np.array(val_summary.read_scalar(\"Top1Accuracy\"))\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(loss[:,0],loss[:,1],label='loss')\n",
    "plt.xlim(0,loss.shape[0]+10)\n",
    "plt.grid(True)\n",
    "plt.title(\"loss\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(top1[:,0],top1[:,1],label='top1')\n",
    "plt.xlim(0,loss.shape[0]+10)\n",
    "plt.title(\"top1 accuracy\")\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trained_model.predict(iris_rdd_test).collect()\n",
    "\n",
    "def map_predict_label(l):\n",
    "    return np.array(l).argmax()\n",
    "def map_groundtruth_label(l):\n",
    "    return l.to_ndarray()[0] - 1\n",
    "\n",
    "y_pred = np.array([ map_predict_label(s) for s in predictions])\n",
    "\n",
    "y_true = np.array([map_groundtruth_label(s.label) for s in iris_rdd_test.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"The prediction accuracy is %.2f%%\"%(acc*100))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm.shape\n",
    "df_cm = pd.DataFrame(cm)\n",
    "plt.figure(figsize = (10,8))\n",
    "sn.heatmap(df_cm, annot=True,fmt='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
