{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "from optparse import OptionParser\n",
    "\n",
    "from bigdl.dataset import base\n",
    "from bigdl.dataset import sentence\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.util.common import Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_data(dest_dir):\n",
    "    TINYSHAKESPEARE_URL = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'  # noqa\n",
    "    file_name = \"input.txt\"\n",
    "    file_abs_path = base.maybe_download(file_name, dest_dir, TINYSHAKESPEARE_URL)\n",
    "    return file_abs_path\n",
    "\n",
    "def prepare_data(sc, folder, vocabsize, training_split):\n",
    "    if not folder.startswith( 'hdfs://' ):\n",
    "        file = download_data(folder)\n",
    "    else:\n",
    "        file = folder\n",
    "    sentences_rdd = sc.textFile(file) \\\n",
    "        .map(lambda line: sentence.sentences_split(line))\n",
    "    pad_sent = sentences_rdd.flatMap(lambda x: x). \\\n",
    "        map(lambda sent: sentence.sentences_bipadding(sent))\n",
    "    tokens = pad_sent.map(lambda pad: sentence.sentence_tokenizer(pad))\n",
    "    train_tokens, val_tokens = tokens.randomSplit([training_split, 1 - training_split])\n",
    "    train_tokens.cache()\n",
    "    val_tokens.cache()\n",
    "\n",
    "    train_max_len = train_tokens.map(lambda x: len(x)).max()\n",
    "    print(\"max length %s\" % train_max_len)\n",
    "\n",
    "    words = train_tokens.flatMap(lambda x: x)\n",
    "    print(\"%s words and %s sentences processed in train data\" % (words.count(), train_tokens.count()))\n",
    "\n",
    "    val_max_len = val_tokens.map(lambda x: len(x)).max()\n",
    "    print(\"val max length %s\" % val_max_len)\n",
    "\n",
    "    val_words = val_tokens.flatMap(lambda x: x)\n",
    "    print(\"%s words and %s sentences processed in validation data\" % (val_words.count(), val_tokens.count()))\n",
    "\n",
    "    sort_words = words.map(lambda w: (w, 1)) \\\n",
    "                .reduceByKey(lambda a, b: a + b) \\\n",
    "                .sortBy(lambda w_c: w_c[1])\n",
    "    vocabulary = np.array(sort_words.map(lambda w: w[0]).collect())\n",
    "\n",
    "    fre_len = vocabulary.size\n",
    "    if vocabsize > fre_len:\n",
    "        length = fre_len\n",
    "    else:\n",
    "        length = vocabsize\n",
    "    discard_vocab = vocabulary[: fre_len-length]\n",
    "    used_vocab = vocabulary[fre_len-length: fre_len]\n",
    "    used_vocab_size = used_vocab.size\n",
    "    index = np.arange(used_vocab_size)\n",
    "    index2word = dict(enumerate(used_vocab))\n",
    "    word2index = dict(zip(used_vocab, index))\n",
    "    total_vocab_len = used_vocab_size + 1\n",
    "    startIdx = word2index.get(\"SENTENCESTART\")\n",
    "    endIdx = word2index.get(\"SENTENCEEND\")\n",
    "\n",
    "    def text2labeled(sent):\n",
    "        indexes = [word2index.get(x, used_vocab_size) for x in sent]\n",
    "        data = indexes[0: -1]\n",
    "        label = indexes[1: len(indexes)]\n",
    "        return data, label\n",
    "\n",
    "    def labeled2onehotformat(labeled_sent):\n",
    "        label = [x+1 for x in labeled_sent[1]]\n",
    "        size = len(labeled_sent[0])\n",
    "        feature_onehot = np.zeros(size * total_vocab_len, dtype='int').reshape(\n",
    "            [size, total_vocab_len])\n",
    "        for i, el in enumerate(labeled_sent[0]):\n",
    "            feature_onehot[i, el] = 1\n",
    "        return feature_onehot, label\n",
    "\n",
    "    def padding(features, label, length):\n",
    "        pad_len = length - len(label)\n",
    "        padded_label = (label + [startIdx] * length)[:length]\n",
    "        feature_padding = np.zeros((pad_len, total_vocab_len), dtype=np.int)\n",
    "        feature_padding[:, endIdx + 1] = np.ones(pad_len)\n",
    "        padded_feautres = np.concatenate((features, feature_padding), axis=0)\n",
    "        return padded_feautres, padded_label\n",
    "\n",
    "    sample_rdd = train_tokens.map(lambda sentence_te: text2labeled(sentence_te)) \\\n",
    "        .map(lambda labeled_sent: labeled2onehotformat(labeled_sent)) \\\n",
    "        .map(lambda x: padding(x[0], x[1], train_max_len)) \\\n",
    "        .map(lambda vectors_label: Sample.from_ndarray(vectors_label[0],\n",
    "                                                       np.array(vectors_label[1]))).cache()\n",
    "\n",
    "    val_sample_rdd = val_tokens.map(lambda sentence_t: text2labeled(sentence_t)) \\\n",
    "        .map(lambda labeled_sent: labeled2onehotformat(labeled_sent)) \\\n",
    "        .map(lambda x: padding(x[0], x[1], val_max_len)) \\\n",
    "        .map(lambda vectors_label: Sample.from_ndarray(vectors_label[0],\n",
    "                                                       np.array(vectors_label[1]))).cache()\n",
    "\n",
    "    return sample_rdd, val_sample_rdd, total_vocab_len\n",
    "\n",
    "def build_model(input_size, hidden_size, output_size):\n",
    "    model = Sequential()\n",
    "    model.add(Recurrent()\n",
    "              .add(RnnCell(input_size, hidden_size, Tanh())))\\\n",
    "        .add(TimeDistributed(Linear(hidden_size, output_size)))\n",
    "    model.reset()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "learningrate = 0.1\n",
    "momentum = 0.0\n",
    "weight_decay = 0.0\n",
    "dampening = 0.0\n",
    "hidden_size = 40\n",
    "vob_size = 4000\n",
    "max_epoch = 30\n",
    "folder = \"/tmp/rnn\"\n",
    "training_split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redire_spark_logs()\n",
    "show_bigdl_info_logs()\n",
    "init_engine()\n",
    "\n",
    "(train_rdd, val_rdd, vob_size) = prepare_data(sc, folder, vob_size, training_split)\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    model=build_model(vob_size, hidden_size, vob_size),\n",
    "    training_rdd=train_rdd,\n",
    "    criterion=TimeDistributedCriterion(CrossEntropyCriterion(), size_average=True),\n",
    "    batch_size=batch_size,\n",
    "    optim_method=SGD(learningrate=learningrate, weightdecay=weight_decay,\n",
    "                        momentum=momentum, dampening=dampening),\n",
    "    end_trigger=MaxEpoch(max_epoch)\n",
    ")\n",
    "\n",
    "optimizer.set_validation(\n",
    "    batch_size=batch_size,\n",
    "    val_rdd=val_rdd,\n",
    "    trigger=EveryEpoch(),\n",
    "    val_method=[Loss(TimeDistributedCriterion(CrossEntropyCriterion(), size_average=True))]\n",
    ")\n",
    "\n",
    "train_model = optimizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
