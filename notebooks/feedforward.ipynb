{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas\n",
    "import datetime as dt\n",
    "\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.dataset.base import *\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from utils import *\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "training_epochs = 15\n",
    "batch_size = 16\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 4\n",
    "n_classes = 3\n",
    "n_hidden_1 = 10 # 1st layer number of features\n",
    "n_hidden_2 = 10 # 2nd layer number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_training = spark.read.csv(\"../data/iris2/iris_training.csv\", header=True, inferSchema=\"true\", mode=\"DROPMALFORMED\")\n",
    "iris_test = spark.read.csv(\"../data/iris2/iris_test.csv\", header=True, inferSchema=\"true\", mode=\"DROPMALFORMED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_training = iris_training.select([col(c).cast(\"double\") for c in iris_training.columns])\n",
    "iris_test = iris_test.select([col(c).cast(\"double\") for c in iris_test.columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(feat1=6.4, feat2=2.8, feat3=5.6, feat4=2.2, label=3.0),\n",
       " Row(feat1=5.0, feat2=2.3, feat3=3.3, feat4=1.0, label=2.0),\n",
       " Row(feat1=4.9, feat2=2.5, feat3=4.5, feat4=1.7, label=3.0),\n",
       " Row(feat1=4.9, feat2=3.1, feat3=1.5, feat4=0.1, label=1.0),\n",
       " Row(feat1=5.7, feat2=3.8, feat3=1.7, feat4=0.3, label=1.0),\n",
       " Row(feat1=4.4, feat2=3.2, feat3=1.3, feat4=0.2, label=1.0),\n",
       " Row(feat1=5.4, feat2=3.4, feat3=1.5, feat4=0.4, label=1.0),\n",
       " Row(feat1=6.9, feat2=3.1, feat3=5.1, feat4=2.3, label=3.0),\n",
       " Row(feat1=6.7, feat2=3.1, feat3=4.4, feat4=1.4, label=2.0),\n",
       " Row(feat1=5.1, feat2=3.7, feat3=1.5, feat4=0.4, label=1.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_training.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+\n",
      "|feat1|feat2|feat3|feat4|label|\n",
      "+-----+-----+-----+-----+-----+\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_test.filter('label == 0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_k_train = iris_training.rdd.map(list)\n",
    "iris_k_test = iris_test.rdd.map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6.4, 2.8, 5.6, 2.2, 3.0],\n",
       " [5.0, 2.3, 3.3, 1.0, 2.0],\n",
       " [4.9, 2.5, 4.5, 1.7, 3.0],\n",
       " [4.9, 3.1, 1.5, 0.1, 1.0],\n",
       " [5.7, 3.8, 1.7, 0.3, 1.0],\n",
       " [4.4, 3.2, 1.3, 0.2, 1.0],\n",
       " [5.4, 3.4, 1.5, 0.4, 1.0],\n",
       " [6.9, 3.1, 5.1, 2.3, 3.0],\n",
       " [6.7, 3.1, 4.4, 1.4, 2.0],\n",
       " [5.1, 3.7, 1.5, 0.4, 1.0],\n",
       " [5.2, 2.7, 3.9, 1.4, 2.0],\n",
       " [6.9, 3.1, 4.9, 1.5, 2.0],\n",
       " [5.8, 4.0, 1.2, 0.2, 1.0],\n",
       " [5.4, 3.9, 1.7, 0.4, 1.0],\n",
       " [7.7, 3.8, 6.7, 2.2, 3.0],\n",
       " [6.3, 3.3, 4.7, 1.6, 2.0],\n",
       " [6.8, 3.2, 5.9, 2.3, 3.0],\n",
       " [7.6, 3.0, 6.6, 2.1, 3.0],\n",
       " [6.4, 3.2, 5.3, 2.3, 3.0],\n",
       " [5.7, 4.4, 1.5, 0.4, 1.0],\n",
       " [6.7, 3.3, 5.7, 2.1, 3.0],\n",
       " [6.4, 2.8, 5.6, 2.1, 3.0],\n",
       " [5.4, 3.9, 1.3, 0.4, 1.0],\n",
       " [6.1, 2.6, 5.6, 1.4, 3.0],\n",
       " [7.2, 3.0, 5.8, 1.6, 3.0],\n",
       " [5.2, 3.5, 1.5, 0.2, 1.0],\n",
       " [5.8, 2.6, 4.0, 1.2, 2.0],\n",
       " [5.9, 3.0, 5.1, 1.8, 3.0],\n",
       " [5.4, 3.0, 4.5, 1.5, 2.0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_k_train.take(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert ndarray data into RDD[Sample]\n",
    "def array2rdd(ds):\n",
    "    #build Sample from ndarrays\n",
    "    def build_sample(c0,c1,c2,c3,prediction):\n",
    "        feature = np.array([c0,c1, c2, c3]).flatten()\n",
    "        label = np.array(prediction)\n",
    "        return Sample.from_ndarray(feature, label)\n",
    "    rdd = ds.map(lambda (c0,c1,c2,c3,prediction): build_sample(c0,c1,c2,c3,prediction))\n",
    "    return rdd\n",
    "\n",
    "iris_rdd_train = array2rdd(iris_k_train)\n",
    "iris_rdd_train.cache()\n",
    "iris_rdd_train.count()\n",
    "\n",
    "iris_rdd_test = array2rdd(iris_k_test)\n",
    "iris_rdd_test.cache()\n",
    "iris_rdd_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sample: features: [JTensor: storage: [ 5.9000001   3.          4.19999981  1.5       ], shape: [4], float], label: JTensor: storage: [ 2.], shape: [1], float"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_rdd_test.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createLinear\n",
      "creating: createReLU\n",
      "creating: createLinear\n",
      "creating: createReLU\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "\n",
    "def multilayer_perceptron(n_hidden_1, n_hidden_2, n_input, n_classes):\n",
    "    # Initialize a sequential container\n",
    "    model = Sequential()\n",
    "    # Hidden layer with ReLu activation\n",
    "    #model.add(Reshape([28*28]))\n",
    "    model.add(Linear(n_input, n_hidden_1).set_name('mlp_fc1'))\n",
    "    model.add(ReLU())\n",
    "    # Hidden layer with ReLu activation\n",
    "    model.add(Linear(n_hidden_1, n_hidden_2).set_name('mlp_fc2'))\n",
    "    model.add(ReLU())\n",
    "    # output layer\n",
    "    model.add(Linear(n_hidden_2, n_classes).set_name('mlp_fc3'))\n",
    "    model.add(LogSoftMax())\n",
    "    return model\n",
    "\n",
    "model = multilayer_perceptron(n_hidden_1, n_hidden_2, n_input, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createClassNLLCriterion\n",
      "creating: createDefault\n",
      "creating: createSGD\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n",
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n",
      "creating: createTrainSummary\n",
      "creating: createSeveralIteration\n",
      "creating: createValidationSummary\n",
      "('saving logs to ', 'multilayer_perceptron-20171103-022007')\n"
     ]
    }
   ],
   "source": [
    "# Create an Optimizer\n",
    "optimizer = Optimizer(\n",
    "    model=model,\n",
    "    training_rdd=iris_rdd_train,\n",
    "    criterion=ClassNLLCriterion(),\n",
    "    optim_method=SGD(learningrate=learning_rate),\n",
    "    end_trigger=MaxEpoch(training_epochs),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "# Set the validation logic\n",
    "optimizer.set_validation(\n",
    "    batch_size=batch_size,\n",
    "    val_rdd=iris_rdd_test,\n",
    "    trigger=EveryEpoch(),\n",
    "    val_method=[Top1Accuracy()]\n",
    ")\n",
    "\n",
    "app_name='multilayer_perceptron-'+dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_summary = TrainSummary(log_dir='/tmp/bigdl_summaries',\n",
    "                                     app_name=app_name)\n",
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(50))\n",
    "val_summary = ValidationSummary(log_dir='/tmp/bigdl_summaries',\n",
    "                                        app_name=app_name)\n",
    "optimizer.set_train_summary(train_summary)\n",
    "optimizer.set_val_summary(val_summary)\n",
    "print(\"saving logs to \",app_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o256.optimize.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 80.0 failed 1 times, most recent failure: Lost task 0.0 in stage 80.0 (TID 45, localhost, executor driver): java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: Boxed Error\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$11.apply(DistriOptimizer.scala:235)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$11.apply(DistriOptimizer.scala:235)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8.apply(DistriOptimizer.scala:235)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8.apply(DistriOptimizer.scala:175)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.ExecutionException: Boxed Error\n\tat scala.concurrent.impl.Promise$.resolver(Promise.scala:55)\n\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:47)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:244)\n\tat scala.concurrent.Promise$class.complete(Promise.scala:55)\n\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anon$1.execute(ThreadPool.scala:203)\n\tat scala.concurrent.impl.Future$.apply(Future.scala:31)\n\tat scala.concurrent.Future$.apply(Future.scala:494)\n\tat com.intel.analytics.bigdl.utils.ThreadPool.invoke(ThreadPool.scala:168)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:107)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:60)\n\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:70)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:224)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply(DistriOptimizer.scala:216)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply(DistriOptimizer.scala:216)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$4.call(ThreadPool.scala:112)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.lang.AssertionError: assertion failed: curTarget 0 is out of range 1 to 3\n\tat scala.Predef$.assert(Predef.scala:170)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion$$anonfun$updateOutput$5.apply(ClassNLLCriterion.scala:109)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion$$anonfun$updateOutput$5.apply(ClassNLLCriterion.scala:107)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:162)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:285)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:795)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: Boxed Error\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$11.apply(DistriOptimizer.scala:235)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$11.apply(DistriOptimizer.scala:235)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8.apply(DistriOptimizer.scala:235)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8.apply(DistriOptimizer.scala:175)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.util.concurrent.ExecutionException: Boxed Error\n\tat scala.concurrent.impl.Promise$.resolver(Promise.scala:55)\n\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:47)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:244)\n\tat scala.concurrent.Promise$class.complete(Promise.scala:55)\n\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anon$1.execute(ThreadPool.scala:203)\n\tat scala.concurrent.impl.Future$.apply(Future.scala:31)\n\tat scala.concurrent.Future$.apply(Future.scala:494)\n\tat com.intel.analytics.bigdl.utils.ThreadPool.invoke(ThreadPool.scala:168)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:107)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:60)\n\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:70)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:224)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply(DistriOptimizer.scala:216)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply(DistriOptimizer.scala:216)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$4.call(ThreadPool.scala:112)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.lang.AssertionError: assertion failed: curTarget 0 is out of range 1 to 3\n\tat scala.Predef$.assert(Predef.scala:170)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion$$anonfun$updateOutput$5.apply(ClassNLLCriterion.scala:109)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion$$anonfun$updateOutput$5.apply(ClassNLLCriterion.scala:107)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:162)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c648da557482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'# Boot training process\\ntrained_model = optimizer.optimize()\\nprint(\"Optimization Done.\")'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py27/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py27/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-596c116e-1792-4af2-a00f-7b6bef8e227c/userFiles-2563b7b9-bc3f-4d9e-a897-f855f4d1bf09/bigdl-0.3.0-SNAPSHOT-python-api.zip/bigdl/optim/optimizer.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mDo\u001b[0m \u001b[0man\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \"\"\"\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mjmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-596c116e-1792-4af2-a00f-7b6bef8e227c/userFiles-2563b7b9-bc3f-4d9e-a897-f855f4d1bf09/bigdl-0.3.0-SNAPSHOT-python-api.zip/bigdl/util/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o256.optimize.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 80.0 failed 1 times, most recent failure: Lost task 0.0 in stage 80.0 (TID 45, localhost, executor driver): java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: Boxed Error\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$11.apply(DistriOptimizer.scala:235)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$11.apply(DistriOptimizer.scala:235)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8.apply(DistriOptimizer.scala:235)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8.apply(DistriOptimizer.scala:175)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.ExecutionException: Boxed Error\n\tat scala.concurrent.impl.Promise$.resolver(Promise.scala:55)\n\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:47)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:244)\n\tat scala.concurrent.Promise$class.complete(Promise.scala:55)\n\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anon$1.execute(ThreadPool.scala:203)\n\tat scala.concurrent.impl.Future$.apply(Future.scala:31)\n\tat scala.concurrent.Future$.apply(Future.scala:494)\n\tat com.intel.analytics.bigdl.utils.ThreadPool.invoke(ThreadPool.scala:168)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:107)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:60)\n\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:70)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:224)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply(DistriOptimizer.scala:216)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply(DistriOptimizer.scala:216)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$4.call(ThreadPool.scala:112)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.lang.AssertionError: assertion failed: curTarget 0 is out of range 1 to 3\n\tat scala.Predef$.assert(Predef.scala:170)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion$$anonfun$updateOutput$5.apply(ClassNLLCriterion.scala:109)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion$$anonfun$updateOutput$5.apply(ClassNLLCriterion.scala:107)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:162)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:285)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:795)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: Boxed Error\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$11.apply(DistriOptimizer.scala:235)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$11.apply(DistriOptimizer.scala:235)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8.apply(DistriOptimizer.scala:235)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8.apply(DistriOptimizer.scala:175)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.util.concurrent.ExecutionException: Boxed Error\n\tat scala.concurrent.impl.Promise$.resolver(Promise.scala:55)\n\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:47)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:244)\n\tat scala.concurrent.Promise$class.complete(Promise.scala:55)\n\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anon$1.execute(ThreadPool.scala:203)\n\tat scala.concurrent.impl.Future$.apply(Future.scala:31)\n\tat scala.concurrent.Future$.apply(Future.scala:494)\n\tat com.intel.analytics.bigdl.utils.ThreadPool.invoke(ThreadPool.scala:168)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:107)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion.updateOutput(ClassNLLCriterion.scala:60)\n\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:70)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:224)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply(DistriOptimizer.scala:216)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$8$$anonfun$9$$anonfun$apply$2.apply(DistriOptimizer.scala:216)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$4.call(ThreadPool.scala:112)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.lang.AssertionError: assertion failed: curTarget 0 is out of range 1 to 3\n\tat scala.Predef$.assert(Predef.scala:170)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion$$anonfun$updateOutput$5.apply(ClassNLLCriterion.scala:109)\n\tat com.intel.analytics.bigdl.nn.ClassNLLCriterion$$anonfun$updateOutput$5.apply(ClassNLLCriterion.scala:107)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invoke$2.apply(ThreadPool.scala:162)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Boot training process\n",
    "trained_model = optimizer.optimize()\n",
    "print(\"Optimization Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
